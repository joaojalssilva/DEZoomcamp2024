{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00bc6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd4a0f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/04 16:00:47 WARN Utils: Your hostname, codespaces-e41708 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "24/03/04 16:00:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/04 16:00:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/04 16:00:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb3e4c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/04 16:01:02 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f53c3aef-b604-4a89-be03-789661c819d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-04 15:55:03--  https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/fhv/fhv_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘fhv_tripdata_2019-10.csv.gz’\n",
      "\n",
      "fhv_tripdata_2019-1     [ <=>                ] 152.56K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2024-03-04 15:55:04 (3.28 MB/s) - ‘fhv_tripdata_2019-10.csv.gz’ saved [156217]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/fhv/fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38a45d69-4106-4e73-a748-4175499a2650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gzip: fhv_tripdata_2019-10.csv.gz: not in gzip format\n"
     ]
    }
   ],
   "source": [
    "!gzip -dc fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5236cebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 38M\n",
      "-rw-r--r-- 1 codespace codespace    0 Mar  4 16:02 _SUCCESS\n",
      "-rw-r--r-- 1 codespace codespace 6.3M Mar  4 16:02 part-00000-f6f1ba9b-b56d-4599-a1f6-53cac091c355-c000.snappy.parquet\n",
      "-rw-r--r-- 1 codespace codespace 6.3M Mar  4 16:02 part-00001-f6f1ba9b-b56d-4599-a1f6-53cac091c355-c000.snappy.parquet\n",
      "-rw-r--r-- 1 codespace codespace 6.3M Mar  4 16:02 part-00002-f6f1ba9b-b56d-4599-a1f6-53cac091c355-c000.snappy.parquet\n",
      "-rw-r--r-- 1 codespace codespace 6.3M Mar  4 16:02 part-00003-f6f1ba9b-b56d-4599-a1f6-53cac091c355-c000.snappy.parquet\n",
      "-rw-r--r-- 1 codespace codespace 6.3M Mar  4 16:02 part-00004-f6f1ba9b-b56d-4599-a1f6-53cac091c355-c000.snappy.parquet\n",
      "-rw-r--r-- 1 codespace codespace 6.3M Mar  4 16:02 part-00005-f6f1ba9b-b56d-4599-a1f6-53cac091c355-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/pq/fhv/2019/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a3399a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68bc8b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')\n",
    "\n",
    "df = df.repartition(6)\n",
    "\n",
    "df.write.parquet('data/pq/fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58989b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data/pq/fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0878044d-998c-49d5-9a4c-c3b7232e1227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02784|2019-10-01 09:55:38|2019-10-01 10:05:43|          89|          85|   NULL|                  NULL|\n",
      "|              B01315|2019-10-05 15:13:04|2019-10-05 15:19:48|         264|          74|   NULL|                B01315|\n",
      "|              B01984|2019-10-12 17:13:00|2019-10-12 17:40:00|         264|          75|   NULL|                B01984|\n",
      "|              B00310|2019-10-15 10:55:04|2019-10-15 11:00:45|         264|         247|   NULL|                B03047|\n",
      "|              B00932|2019-10-08 06:58:42|2019-10-08 07:11:11|         264|          37|   NULL|                B00932|\n",
      "|              B01029|2019-10-10 14:45:00|2019-10-10 15:47:00|         264|         264|   NULL|                B01029|\n",
      "|              B01087|2019-10-14 18:41:24|2019-10-14 19:02:06|         261|         186|   NULL|                B01087|\n",
      "|              B03080|2019-10-05 14:49:10|2019-10-05 15:02:14|         264|          25|   NULL|                B02889|\n",
      "|              B03160|2019-10-10 12:50:00|2019-10-10 13:34:00|          77|          77|   NULL|                B02882|\n",
      "|              B02472|2019-10-16 14:12:36|2019-10-16 14:35:00|         264|         157|   NULL|                B02472|\n",
      "|              B01051|2019-10-05 22:06:46|2019-10-05 22:16:57|         264|         182|   NULL|                B01051|\n",
      "|              B02111|2019-10-08 14:58:52|2019-10-08 15:40:41|          98|          79|   NULL|                B02111|\n",
      "|              B00254|2019-10-03 20:33:11|2019-10-03 21:52:16|         246|         265|   NULL|                B02356|\n",
      "|              B00756|2019-10-16 10:58:00|2019-10-16 11:18:00|         264|         264|   NULL|                B00756|\n",
      "|              B02249|2019-10-04 19:55:49|2019-10-04 20:08:25|         264|         192|   NULL|                B02249|\n",
      "|              B03202|2019-10-13 07:39:33|2019-10-13 08:18:31|         264|         132|   NULL|                B03202|\n",
      "|              B00419|2019-10-11 08:33:12|2019-10-11 08:46:22|         182|         185|   NULL|                B00419|\n",
      "|              B02095|2019-10-09 11:16:00|2019-10-09 11:44:00|         264|         264|   NULL|                B02095|\n",
      "|              B02930|2019-10-05 22:06:15|2019-10-05 22:25:31|         264|          69|   NULL|                B02930|\n",
      "|              B01239|2019-10-07 20:08:15|2019-10-07 20:16:06|         264|          51|   NULL|                B02847|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7489aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c2500fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .filter(\"pickup_date = '2019-10-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd7ae60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/DEZoomcamp2024/spark-3.5.1-bin-hadoop3/python/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('fhv_2019_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d47c147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhv_2019_10\n",
    "WHERE\n",
    "    to_date(pickup_datetime) = '2019-10-15';\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7befe422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "279d9161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|pickup_date|    max(duration)|\n",
      "+-----------+-----------------+\n",
      "| 2019-10-28|         631152.5|\n",
      "| 2019-10-11|         631152.5|\n",
      "| 2019-10-31|87672.44083333333|\n",
      "| 2019-10-01|70128.02805555555|\n",
      "| 2019-10-17|           8794.0|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('duration', (df.dropoff_datetime.cast('long') - df.pickup_datetime.cast('long'))/3600) \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "        .max('duration') \\\n",
    "    .orderBy('max(duration)', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74cf0e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|          duration|\n",
      "+-----------+------------------+\n",
      "| 2019-10-28|          631152.5|\n",
      "| 2019-10-11|          631152.5|\n",
      "| 2019-10-31| 87672.44083333333|\n",
      "| 2019-10-01| 70128.02805555555|\n",
      "| 2019-10-17|            8794.0|\n",
      "| 2019-10-26| 8784.166666666666|\n",
      "| 2019-10-30|1464.5344444444445|\n",
      "| 2019-10-25|1056.8266666666666|\n",
      "| 2019-10-02| 769.2313888888889|\n",
      "| 2019-10-23| 745.6166666666667|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    to_date(pickup_datetime) AS pickup_date,\n",
    "    MAX((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 3600) AS duration\n",
    "FROM \n",
    "    fhv_2019_10\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25816aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|PULocationID|count(1)|\n",
      "+------------+--------+\n",
      "|           2|       1|\n",
      "|         105|       2|\n",
      "|         111|       5|\n",
      "|          30|       8|\n",
      "|         120|      14|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    PULocationID,\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhv_2019_10\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 ASC\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a78f9fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|PULocationID|count|\n",
      "+------------+-----+\n",
      "|           2|    1|\n",
      "|         105|    2|\n",
      "|         111|    5|\n",
      "|          30|    8|\n",
      "|         120|   14|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .groupBy('PULocationID') \\\n",
    "        .count() \\\n",
    "    .orderBy('count') \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74b7f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81642d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LocationID', 'Borough', 'Zone', 'service_zone']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f460dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad8f0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.registerTempTable('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f738414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------+\n",
      "|                Zone|PULocationID|count(1)|\n",
      "+--------------------+------------+--------+\n",
      "|         Jamaica Bay|           2|       1|\n",
      "|Governor's Island...|         105|       2|\n",
      "| Green-Wood Cemetery|         111|       5|\n",
      "|       Broad Channel|          30|       8|\n",
      "|     Highbridge Park|         120|      14|\n",
      "+--------------------+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    pul.Zone,\n",
    "    PULocationID,\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhv_2019_10 fhv LEFT JOIN zones pul ON fhv.PULocationID = pul.LocationID\n",
    "                      LEFT JOIN zones dol ON fhv.DOLocationID = dol.LocationID\n",
    "GROUP BY\n",
    "    1, 2\n",
    "ORDER BY\n",
    "    3 ASC\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b754d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
